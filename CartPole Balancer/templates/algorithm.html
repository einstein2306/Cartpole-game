<!DOCTYPE html>
<html lang="en">
<head>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script> 
    <link href="/static/css/algorithm.css" rel="stylesheet">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Algorithm</title>
</head>
<body>
    <h1>DQN Algorithm</h1>
    <hr>
    <p style="text-indent:40px;">DQN stands for <i>Deep Q-network</i>, It combines the principles of deep neural networks with Q-learning.
       Q-learning algorithm is the Off-Policy algorithm which is the training policy is not equal to target policy 
       to update the values. Deep Q-Network (DQN) is a powerful algorithm in the field of reinforcement learning. while
       enabling agents to learn optimal policies in complex environments. Deep Q network (DQN) builds on Fitted Q-Iteration (FQI)
       and make use of different tricks to stabilize the learning with neural network which uses a replay buffer, a target network
       and gradient clipping.
    </p>

    <h5>Tabular information of compatibility:</h5>
    <ul>
        <li>Recurrent policies: &#x274C;</li>
        <li>Multi processing: &#x2714;</li>
        <li>Gym space: </li>
    </ul>
    <table class="table table-bordered table-striped table-sm mx-auto" style="width:50%;">
        <thead>
            <tr>
                <th scope="col">Space</th>
                <th scope="col">Action</th>
                <th scope="col">Observation</th>
            </tr> 
        </thead>
        <tbody>
            <tr>
                <td>Discrete</td>
                <td>&#x2714;</td>
                <td>&#x2714;</td>
            </tr>
            <tr>
                <td>Box</td>     
                <td>&#x274C;</td> 
                <td>&#x2714;</td>        
            </tr>
            <tr>
                <td>MultiDiscrete</td>     
                <td>&#x274C;</td> 
                <td>&#x2714;</td>         
            </tr>
            <tr>
                <td>MultiBinary</td>     
                <td>&#x274C;</td> 
                <td>&#x2714;</td>         
            </tr>
            <tr>
                <td>Dict</td>     
                <td>&#x274C;</td> 
                <td>&#x2714;</td>         
            </tr>
        </tbody>
    </table><br>
    <h5>Notes:</h5>
    <ul>
        <li>Original paper: <a href="https://arxiv.org/abs/1312.5602" style="text-decoration: none;">https://arxiv.org/abs/1312.5602</a></li>
        <li>Further Reference: <a href="https://www.nature.com/articles/nature14236" style="text-decoration: none;">https://www.nature.com/articles/nature14236</a></li>
        <li>Tutorial "From Tabular Q-Learning to DQN":  <a href="https://github.com/araffin/rlss23-dqn-tutorial" style="text-decoration:none">https://github.com/araffin/rlss23-dqn-tutorial</a></li>
    </ul>
    
    
    
</body>
</html>
    
