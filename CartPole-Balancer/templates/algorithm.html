<!DOCTYPE html>
<html lang="en">
<head>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script> 
    
    <link href="/static/css/algorithm.css" rel="stylesheet">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Algorithm</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <h1>DQN Algorithm</h1>
    <hr>
    <p style="text-indent:40px;">DQN stands for <i>Deep Q-network</i>, It combines the principles of deep neural networks with Q-learning.
       Q-learning algorithm is the Off-Policy algorithm which is the training policy is not equal to target policy 
       to update the values. Deep Q-Network (DQN) is a powerful algorithm in the field of reinforcement learning. while
       enabling agents to learn optimal policies in complex environments. Deep Q network (DQN) builds on Fitted Q-Iteration (FQI)
       and make use of different tricks to stabilize the learning with neural network which uses a replay buffer, a target network
       and gradient clipping.
    </p>

    <h5>Tabular information of compatibility:</h5>
    <ul>
        <li>Recurrent policies: &#x274C;</li>
        <li>Multi processing: &#x2714;</li>
        <li>Gym space: </li>
    </ul>
    <table class="table table-bordered table-striped table-sm mx-auto" style="width:50%;">
        <thead>
            <tr>
                <th scope="col">Space</th>
                <th scope="col">Action</th>
                <th scope="col">Observation</th>
            </tr> 
        </thead>
        <tbody>
            <tr>
                <td>Discrete</td>
                <td>&#x2714;</td>
                <td>&#x2714;</td>
            </tr>
            <tr>
                <td>Box</td>     
                <td>&#x274C;</td> 
                <td>&#x2714;</td>        
            </tr>
            <tr>
                <td>MultiDiscrete</td>     
                <td>&#x274C;</td> 
                <td>&#x2714;</td>         
            </tr>
            <tr>
                <td>MultiBinary</td>     
                <td>&#x274C;</td> 
                <td>&#x2714;</td>         
            </tr>
            <tr>
                <td>Dict</td>     
                <td>&#x274C;</td> 
                <td>&#x2714;</td>         
            </tr>
        </tbody>
    </table><br>
    <h5>Notes:</h5>
    <ul>
        <li>Original paper: <a href="https://arxiv.org/abs/1312.5602" style="text-decoration: none;">https://arxiv.org/abs/1312.5602</a></li>
        <li>Further Reference: <a href="https://www.nature.com/articles/nature14236" style="text-decoration: none;">https://www.nature.com/articles/nature14236</a></li>
        <li>Tutorial "From Tabular Q-Learning to DQN":  <a href="https://github.com/araffin/rlss23-dqn-tutorial" style="text-decoration:none">https://github.com/araffin/rlss23-dqn-tutorial</a></li>
    </ul>
    <div>
        <h5>Mathematical Description</h5>
        <p style="text-indent:44px;">Reinforcement learning is based on the concept of rewards and penalties, where each action is associated with a value known as the Q-value. The Q-value can be computed using the Q-function. The discount factor, denoted as \( \gamma \), determines the importance of immediate rewards compared to future rewards.
             Means present rewards must be considered instead of future rewards. Based on the value of the discount factor the rewards will be considered.
             if the value of \(\gamma \) is large value then it consider the future rewards also or if it is less means only consider the immediate rewards.
             <br>
            <div style="text-indent:40%;">
                \( R_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k} \)
            </div>
            Here, \( R_t \) represents the total reward at time step \( t \), and \( \gamma \) is the discount factor. The discounted reward can be expressed in an expanded form as follows:<br>
            <div style="text-indent:40%;">
                \( R_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \ldots \)
            </div>
            The Q-function, which is calculated based on the states \( s \) and actions \( a \) at each time step, estimates the optimal Q-values for each action. It does so by taking \( s \) and \( a \) as input values.<br>
            <div style="text-indent:40%;">
                \( Q(s, a) = \mathbb{E} [R_t | s_t, a_t] \)
            </div>
            The Q-function can be computed using the Bellman equation. The loss, denoted as \( L \), is calculated using the predicted Q-values and the best action for the next state. This loss is used to adjust the weights of the neural networks.<br>
            <div style="text-indent:40%;">
                \( L = \left\| r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right\|^2 \)
            </div>
        </p>
    </div>
    
    <hr>
    <p style="text-align: center;font-size: small;">copyright &copy; 2025</p>
</body>
</html>

